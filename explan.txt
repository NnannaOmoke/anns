The basis of a neural network is a perceptron
To build a neural network we connect perceptrons in layers 
Then, each perceptron in a layer is connected to the layer in front of it
The first layer of a neural network is the input layer. The last layer is the output layer
Any layer in between is a hidden layer. They are difficult to interpret.
A deep neural network is one with 2 or more hidden layers
A wide layer has a lot of perceptrons in them. A deep one has lots of layers
Neural networks can approximate any continous function
Constraints of neural networks are activation functions
Common activation functions include:
    tanh
    rectified Linear Unit
    softmax activation function
    sigmoid
The Sequential() creates a sequence of perceptrons. The Dense() basically defines the connections between the perceptrons
Models are compiled with compile(). The choice of optimizer can be "adam" (very popular model), and others.
The loss can be:
    for multi class classification: categorical_cross_entropy
    for binary class classification: binary_cross_entropy
    for regression tasks: mse
Metrics can be the usual metrics used problem-type
when fitting, epochs are the number of times one goes over the dataset
keras models can be saved with the model.save() function. from keras.models.load_model provides a means to load keras models. keras models are usually saved with the .f5 extension
batch sizes will help prevent overfitting, as the learner trains on a seperate batch on each epoch
for binary class classification with deep learners, it is best to use a sigmoid activation function for the output layer
callbacks help prevent overfitting, because they will stop the training after a number of epochs, if some value that its monitoring stops (increasing, decreasing,[dependent on mode]), after a set number of epochs
another means of preventing overfitting is to enable dropout layers, which will turn off a certain percentage of perceptrons in the layer